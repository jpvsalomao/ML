---
title: "Classification"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(ISLR) 
library(ggplot2) 
library(reshape2) 
library(plyr) 
library(dplyr) 
library(class)
```

```{r}
## Reading df
## setwd("/Users/joaopedro/Documents/MSBA/Classes/BAX 452 - Machine Learning/Assignments/05. Classification")
```

```{r}
## Reading the data
wine <- read.csv('winequality-red.csv')
head(wine)
```
```{r}
## Exploring the data
str(wine)
```


**Split the wine into three categories (for low, medium, and high quality)**

To create three categories, we need to explore the distribution of the 'quality'.

```{r}
table(wine$quality)
```

Let's define:

- Low quality: 3, 4, 5

- Medium quality: 6

- High Quality: 7, 8

```{r}
assign_quality <- function(quality) {
                  if (quality < 6) {'low'}
                  else if (quality < 7) {'medium'}
                  else {'high'}
                  }
```

```{r}
wine['quality_group'] <- apply(X = wine['quality'], FUN = assign_quality, MARGIN = 1)
table(wine$quality_group)
```

**Explore the data**

```{r}
## Distribution of quality
hist(wine$quality)
```
The quality distribution is approximatelly normally distributed, ranging between 3 and 8, with the majority of wines in the 5 and 6 bins.

```{r}
## Check all possible correlations
cor_matrix <- round(cor(wine[,1:12]),3)
cor_matrix
```
The quality variable shows a positive correlations (>0.2) with:

- alcohol: 0.48
- sulphates: 0.25
- citric acid: 0.23

And negative correlation (< -0.2) with:

- Volatile acid: -0.39

**Split the data into 80% training and 20% testing.**

```{r}
library(caret)

## Droping quality column
wine <- select(wine, -c(quality))

## Train Test Split
set.seed(123)
train_test <- createDataPartition(y = wine$quality_group, p = 0.8, list = FALSE)
training <- wine[train_test,]
testing <- wine[-train_test,]

## Checking Split
dim(training); dim(testing)
```

```{r}
## Training the model
trControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(123)

knn_fit <- train(quality_group ~., data = training, method = "knn",
 trControl=trControl,
 preProcess = c("center", "scale"),
 tuneLength = 10)

## Model Result
knn_fit
```

From the results, we can see that the best k is 11.

```{r}
## Test prediction
test_pred <- predict(knn_fit, newdata = testing)

## Confusion Matrix
confusionMatrix(test_pred, as.factor(testing$quality_group))
```
From the confusion matrix we see that our model had 0.6038 accuracy.

**Use multinomial logistic regression to classify the same dataset**

```{r}

library(nnet)

# Fitting the multinomial logistic regression
winefit <- multinom(quality_group~., data=training)

summary(winefit)

```

We need to convert the coefficients to the exponents to interpret their effects on the odds ratio. 

```{r}
exp(coef(winefit))
```

We see few variables having different effects on making it low or medium, especially the volatile acidity and chlorides. 

High value of volatile acidity and chlorides increase the odds ratio of it being low by the value shown above. These effects are clearly different for low and medium quality. 

```{r}

boxplot(volatile.acidity~quality_group,
        data=training, main="Volatile Acidity",
        xlab="Quality", ylab="Acidity")

```


```{r}

boxplot(pH~quality_group,
        data=training, main="Chlorides",
        xlab="Quality", ylab="Acidity")

```

Even though the multinomial model says, high chlorides correspond to low quality wine, we don't see a direct correlation from the box plot. 

There are interactions happening within the variables that we need to look further to fine tune this. 

```{r}

test_pred_multinom <- predict(winefit, newdata = testing, "class")

# Building classification table
tab <- table(testing$quality_group, test_pred_multinom)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)

```

We see the model is 66% accurate in predicting the right quality group based on the features considered on a test dataset. 

**K-means clustering**

To start with, we need to scale the variables before we cluster

```{r}

## scale the data
xtraining <- scale(training[,0:11]) 

```

Let's cluster using k-means using k=3.

```{r}

set.seed(23)
wine_quality <- kmeans(xtraining, centers=3, nstart=5) 
print(wine_quality$centers)

```

Cluster 1 has high volatile acidity, high pH (relatively), least fixed acidity, citric acid, chlorides, density and sulphates. Cluster 3 is exactly on the other end of the spectrum based on the above mentioned characteristics of Cluster 1 and Cluster 2 is in between. 

We can't really compare the results of clustering with the supervised approaches because clustering is a unsupervised algorithm and the clusters that are formed aren't necessarily about wine quality as we defined in the supervised cases. 

Clusters are somethings that the model came up with and we need to interpret the characteristics of it and come up with a name for each. 


#### Describe the three approaches (knn, multinomial logistic regression, and k-means) and compare/contrast them with each other.


KNN refers to K-Nearest Neighbors. The intuition of this model is simple. For each point to be classified we:
Look at the classification of K nearest records. Those are the neighbors with similar features.
For classification, we find the classification proportion of the closest records and assign the majority class to the new record.
For regression, we use the average of the K nearest neighbors and predict this value to the new point
KNN is a supervised ML model, so we need a response variable to train the model.


Multinomial Logistic Regression is an extension of the traditional Logistic Regression. It also uses the maximum likelihood estimation (MLE) to estimate the probability of the records belonging to each class, but it includes the possibility of having more than two outcomes.


K-means is a clustering technique used to divide the data into different subgroups. The main objective is to identify meaningful groups of data. K-means do that by minimizing the sum of the squared distance of each point to the mean of its cluster. Unlike KNN, K-means is an unsupervised technique, which means it trains the data without a response variable present


#### Which approach would you recommend?


We see that the k-NN is a naive approach to classify an outcome based on training dataset since it comes with its own limitation of following the majority rule when it’s not about the majority and rather about the proximity to the nearest neighbors.

Multinomial logistic regression does relatively better than the k-NN as expected. With more refinements to the model by doing variable selection, we can improve the model and interpret the results better.

Since clustering is an exploratory approach when we don’t have an outcome class we are interested in, it is not the best approach for this use case. Even though we understand the innate characteristics about different wines and how they compare with each other, it can be used to interpret and refine the multinomial model further. Hence, we would recommend the best way forward is the multinomial approach as it can help us predict better and interpret the influence of individual variables to the outcome.